# Running Ollama Locally

***Student**: Complete below*

## Which model did you choose to run?
The model that we chose to run was gemma3:1b. It is too large to run on the CPU alone, but it is still fast enough to write a haiku in a reasonable amount of time. Because it is being run on just the CPU, a smaller model was the one that we decided on. It isn't the largest model available because of the time it would take to access the RAM using a large model. 
## Paste a haiku from that model. Better be on a fun subject.
***Write a haiku about electrical engineering***
Wires hum and flow,
Logic builds a circuit bright,
Power flows with grace.